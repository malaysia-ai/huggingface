<p align="center">
    <a href="#readme">
        <img alt="logo" width="40%" src="malay-huggingface.png">
    </a>
</p>
<p align="center">
    <a href="https://github.com/huseinzol05/malay-huggingface/blob/master/LICENSE"><img alt="MIT License" src="https://img.shields.io/github/license/huseinzol05/malay-huggingface.svg?color=blue"></a>
    <a href="https://discord.gg/aNzbnRqt3A"><img alt="discord" src="https://img.shields.io/badge/discord%20server-malaya-rgb(118,138,212).svg"></a>
</p>

---

**malaysia-huggingface**, Compile Malaysia AI models in HuggingFace, https://huggingface.co/malay-huggingface

## Why HuggingFace?

1. Straightforward to run. Simply pip install and you are good to go.

2. HuggingFace become a new standard to distribute machine learning models, from text based up to image based.

3. We want developers very easy to run Malay AI models.

## How-to start

1. Install [git-lfs](https://github.com/git-lfs/git-lfs/wiki/Installation).

2. Git clone and you are good to go.

## How-to get access to HuggingFace organization

1. Fork and pull request.

2. Once you become a maintainer, you will get access to https://huggingface.co/malay-huggingface

## Pretrained Models

- **ALBERT**, a Lite BERT for Self-supervised Learning of Language Representations, https://arxiv.org/abs/1909.11942
- **BERT**, Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805
- **BigBird**, Transformers for Longer Sequences, https://arxiv.org/abs/2007.14062
- **ELECTRA**, Pre-training Text Encoders as Discriminators Rather Than Generators, https://arxiv.org/abs/2003.10555
- **GPT2**, Language Models are Unsupervised Multitask Learners, https://github.com/openai/gpt-2
- **PEGASUS**, Pre-training with Extracted Gap-sentences for Abstractive Summarization, https://arxiv.org/abs/1912.08777
- **T5**, Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, https://arxiv.org/abs/1910.10683
- **TinyBERT**, Distilling BERT for Natural Language Understanding, https://arxiv.org/abs/1909.10351
- **XLNET**, Generalized Autoregressive Pretraining for Language Understanding, https://arxiv.org/abs/1906.08237

Feel free to try in huggingface ðŸ¤—, https://huggingface.co/malay-huggingface

## Finetuned Models

- **Pembalakan**, finetuned EfficientNet-Unet on https://github.com/BioWar/Satellite-Image-Segmentation-using-Deep-Learning-for-Deforestation-Detection dataset, deforestation semantic segmentation.

## Acknowledgement

Thanks to [KeyReply](https://www.keyreply.com/) for sponsoring private cloud to train models.

<a href="#readme">
    <img alt="logo" width="20%" src="https://cdn-images-1.medium.com/max/1200/1*j7PuuApAuKhdvRD_5jPvLA@2x.png">
</a>

Also, thanks to [Tensorflow Research Cloud](https://www.tensorflow.org/tfrc) for free TPUs access.

<a href="https://www.tensorflow.org/tfrc">
    <img alt="logo" width="20%" src="https://2.bp.blogspot.com/-xojf3dn8Ngc/WRubNXxUZJI/AAAAAAAAB1A/0W7o1hR_n20QcWyXHXDI1OTo7vXBR8f7QCLcB/s400/image2.png">
</a>