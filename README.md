<p align="center">
    <a href="#readme">
        <img alt="logo" width="40%" src="malay-huggingface.png">
    </a>
</p>
<p align="center">
    <a href="https://github.com/huseinzol05/malay-huggingface/blob/master/LICENSE"><img alt="MIT License" src="https://img.shields.io/github/license/huseinzol05/malay-huggingface.svg?color=blue"></a>
    <a href="https://discord.gg/aNzbnRqt3A"><img alt="discord" src="https://img.shields.io/badge/discord%20server-malaya-rgb(118,138,212).svg"></a>
</p>

---

**malay-huggingface**, Compile Malay NLP models for HuggingFace. 

## Why HuggingFace?

1. Straightforward to run. Simply pip install and you are good to go.

2. HuggingFace become a new standard to distribute NLP models, from text based up to image based.

3. We want developers very easy to run Malay NLP models. I know [Malaya](https://github.com/huseinzol05/malaya) and [Malaya-Speech](https://github.com/huseinzol05/malaya-speech) are very hard to run and not really community friendly.

## Pretrained Models

- **ALBERT**, a Lite BERT for Self-supervised Learning of Language Representations, https://arxiv.org/abs/1909.11942
- **BERT**, Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805
- **BigBird**, Transformers for Longer Sequences, https://arxiv.org/abs/2007.14062
- **ELECTRA**, Pre-training Text Encoders as Discriminators Rather Than Generators, https://arxiv.org/abs/2003.10555
- **GPT2**, Language Models are Unsupervised Multitask Learners, https://github.com/openai/gpt-2
- **PEGASUS**, Pre-training with Extracted Gap-sentences for Abstractive Summarization, https://arxiv.org/abs/1912.08777
- **T5**, Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, https://arxiv.org/abs/1910.10683
- **TinyBERT**, Distilling BERT for Natural Language Understanding, https://arxiv.org/abs/1909.10351
- **XLNET**, Generalized Autoregressive Pretraining for Language Understanding, https://arxiv.org/abs/1906.08237

Or can try use in huggingface ðŸ¤—, https://huggingface.co/malay-huggingface

## Why under huseinzol05?

1. If others interested to help, we can create a public organization for this and future repositories.

## Acknowledgement

Thanks to [KeyReply](https://www.keyreply.com/) for sponsoring private cloud to train Malaya-Speech models, without it, this library will collapse entirely.

<a href="#readme">
    <img alt="logo" width="20%" src="https://cdn.techinasia.com/data/images/16234a59ae3f218dc03815a08eaab483.png">
</a>

Also, thanks to [Tensorflow Research Cloud](https://www.tensorflow.org/tfrc) for free TPUs access.

<a href="https://www.tensorflow.org/tfrc">
    <img alt="logo" width="20%" src="https://2.bp.blogspot.com/-xojf3dn8Ngc/WRubNXxUZJI/AAAAAAAAB1A/0W7o1hR_n20QcWyXHXDI1OTo7vXBR8f7QCLcB/s400/image2.png">
</a>